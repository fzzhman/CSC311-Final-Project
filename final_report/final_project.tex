\documentclass{article}
\usepackage[gobble=auto]{pythontex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{bbm}
\usepackage{graphicx}

\setpythontexfv{breaklines}
\renewcommand\thesubsection{\alph{subsection}}

\title{CSC311 - Final Assignment}
\author{Zelong Liu, Fizzah Mansoor, Harrison Deng}

\begin{document}
    \begin{pylabcode}
        import glob
        import shutil
        import os
        ### Setting up PythonTex environment. ###

        # Copy necessary code #
        for p in glob.glob("../code/**/*.py"):
            pytex.add_dependencies(p)
        
        if os.path.exists("./codebase"):
            shutil.rmtree("./codebase")
        shutil.copytree("../code/", "./codebase")
        for p in glob.glob("./codebase/**/*"):
            pytex.add_created(p)

        # Setting up automatic graphic inclusion #
        fig_info = {
            "dir": "./figures/generated/",
            "basename": '_'.join([pytex.session, pytex.family, pytex.restart]),
            "num": 0
        }
        pytex.figures = fig_info

        def gen_figure():
            figures = pytex.figures
            for num in get_fignums():
                name = figures["basename"] + "_fig" + str(figures["num"]) + ".pdf"
                path = figures["dir"] + name
                figures["num"] = figures["num"] + 1
                figure(num)
                savefig(path)
                close(num)
                pytex.add_created(path)
            return "\\includegraphics{" + path + "}"
    \end{pylabcode}


    \maketitle

    \pagebreak

    \tableofcontents

    \pagebreak
    
    \part{Predicting Student Correctness}
    
    \section{K-Nearest Neighbor}
    \subsection{Complete Main kNN, Plot and Report Accuracy}
    The implementation of all code is in the \verb|part_a/knn.py| file. Following are plots of accuracy on validation data as a function of $k \in \{1,6,11,16,21,26\}$:

    \begin{pylabblock}
        import codebase.part_a.knn as knn
        k_vals, val_user_acc, val_item_acc = knn.main("./codebase/data")
    \end{pylabblock}

    \begin{pylabblock}
        knn.accuracy_plot(k_vals, val_user_acc, "User-Based Collaborative Filtering")
    \end{pylabblock}
    \pylab{gen_figure()}

    \begin{pylabblock}
        knn.accuracy_plot(k_vals, val_item_acc, "Item-Based Collaborative Filtering")
    \end{pylabblock}
    \pylab{gen_figure()}

    


    \subsection{Selecting k*}
    
    \subsection{Implementing Impute by Item}
    Underlying assumption: if answers by certain users to Question A match those of Question B, then Aâ€™s answer correctness corresponding to a specific user matches that of question Y. 

    \subsection{Comparing user and item based Collaborative Filtering}
    User-Based collaborative filtering performs better on test data. $68.416\%$ accuracy on user-based filtering and $68.162\%$ accuracy on item-based filtering.

    \subsection{Potential Limitations of kNN in this Context}
    We can safely assume that there is a high correlation between both question difficulty and student ability on whether or not the question was answered correctly. But, feature importance is not possible for the KNN algorithm (there is no way to define the features which are responsible for the classification), so it will not be able to make accurate inferences based on these two parameters. In the algorithm used in this question, either one of the two parameters (user ability or question difficulty) is focused on, so it has lower validation and test accuracy scores than other algorithms in Part A of this project. 

    KNN runs slowly. Finding the optimal k-value from the given list of possible k values ({1, 6, 11, 21, 26}) takes several minutes for each function.

    


    \pagebreak

    \section{Item Response Theory}
    \subsection{Mathematical Derivations for IRT}
    We are given that $p(c_{ij} = 1 \vert \bm{\theta}, \bm{\beta})$. We will assume $c_{ij}$ is a value in $\bm{C}$ where $i$ and $j$ as coordinates are in set $O$ as defined:
    \[ O = \{(i,j): \text{Entry $(i,j)$ of matrix $\bm{C}$ and is observed}\} \].

    Since this $c_{ij}$ is a binary value, we can describe $P(\bm{C} \vert \bm{\theta}, \bm{\beta})$ with a bernoulli distribution:
    \[p(C \vert \bm{\theta}, \bm{\beta}) = \prod_{ij}[\frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}]^{c_{ij}}[\frac{1}{1 + exp(\theta_{i} - \beta_{j})}]^{(1-c_{ij})}\]

    Therefore, our Likelihood function is:
    \[L(\bm{\theta}, \bm{\beta}) =\prod_{ij}[\frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}]^{c_{ij}}[\frac{1}{1 + exp(\theta_{i} - \beta_{j})}]^{(1-c_{ij})}\]

    Then, apply log to obtain the log-likelihood where $N$ and $M$ are the number of users and questions respectively:
    \begin{align*}
        L(\bm{\theta}, \bm{\beta}) &=\prod_{ij}[\frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}]^{c_{ij}}[\frac{1}{1 + exp(\theta_{i} - \beta_{j})}]^{(1-c_{ij})} \\
        log(L(\bm{\theta}, \bm{\beta})) &= \log(\prod_{ij}[\frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}^{c_{ij}}][\frac{1}{1 + exp(\theta_{i} - \beta_{j})}^{1-c_{ij}}] \\
        &=\sum_{i=1}^{N} \sum_{j=1}^{M} \log([\frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}^{c_{ij}}][\frac{1}{1 + exp(\theta_{i} - \beta_{j})}^{1-c_{ij}}]) \\
        &=\sum_{i=1}^{N} \sum_{j=1}^{M} c_{ij}((log(exp(\theta_{i} - \beta_{j})) - log(1 + exp(\theta_{i} - \beta_{j}))) \\
        &+ (1 - c_{ij})(\log(1) - \log(1 + exp(\theta_{i} - \beta_{j}))) \\
        &=\sum_{i=1}^{N} \sum_{j=1}^{M} [c_{ij}(\theta_{i} - \beta_{j}) - \log(\frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})})] \\
    \end{align*}

    Then, we solve for the partial derivative with respect to $\theta_i$ and $\beta_j$ respectively:
    \begin{align*}
        \frac{\delta}{\delta\theta_{i}} &=  \sum_{j=1}^{M}{[c_{ij} - \frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}]} \\
        \frac{\delta}{\delta\beta_{j}} &= \sum_{i=1}^{N}{[-c_{ij} + \frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}]}
    \end{align*}
    
    \subsection{Implementation of IRT}
    The implementation of IRT is in \verb|part_a/item_response.py|. We chose the hyperparameters $\alpha$ and iterations number by performing multiple combinations of them and seeing which one had the highest validation score (automated, see mentioned code file for this automation). We then manually adjusted the set of tested values and repeated. Doing this a few times resulted in:

    \begin{pylabblock}
        import codebase.part_a.item_response as irt
        print()
        irt_results = irt.main("./codebase/data")
    \end{pylabblock}
    \printpythontex[verb]

    \medskip

    \noindent
    The following is a plot of the training curve (and the code used to generate it):
    \begin{pylabblock}
        plot(irt_results["train_nllks"], label="train")
        plot(irt_results["val_nllks"], label="valid")
        ylabel("Negative Log Likelihood")
        xlabel("Iteration")
        title("Neg Log Likelihood for Train and Validation Data")
        legend()
    \end{pylabblock}
    \pylab{gen_figure()}

    \subsection{Reporting Validation and Test Accuracies}

    Validation and test accuracies have been calculated in the previous call to the main function. Again, implementation is in \verb|part_a/item_response.py|.

    \medskip

    \noindent
    Our validation accuracy:
    \begin{pylabblock}
        print(irt_results["val_acc"])
    \end{pylabblock}
    \printpythontex[verb]

    \medskip

    \noindent
    Test accuracy:
    \begin{pylabblock}
        print(irt_results["test_acc"])
    \end{pylabblock}
    \printpythontex[verb]

    \subsection{Plots of Questions With Respect to $\bm{\theta}$ and $\bm{\beta}$}

    \pagebreak

    \section{Neural Networks}
    \subsection{Differences Between ALS and Neural Networks}
    \subsection{Implementing AutoEncoder}
    \subsection{Tuning and Training NN}
    \subsection{Ploting and Reporting}
    \subsection{Implementing $L_2$ Regularization}

    \pagebreak

    \section{Ensemble}

    \pagebreak

    \part{Modifying for Higher Accuracy}
    \section{Formal Description}
    \section{Figure or Diagram}
    \section{Comparison or Demonstration}
\end{document}