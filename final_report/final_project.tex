\documentclass{article}
\usepackage[gobble=auto]{pythontex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{bbm}
\usepackage{graphicx}

\fvset{breaklines}
\renewcommand\thesubsection{\alph{subsection}}

\title{CSC311 - Final Assignment}
\author{Zelong Liu, Fizzah Mansoor, Harrison Deng}

\pylabc[KNN]{from fig_gen_utils import *}
\pylabc[IRT]{from fig_gen_utils import *}
\pylabc[NN]{from fig_gen_utils import *}
\pylabc[ENSEMBLE]{from fig_gen_utils import *}

\begin{document}

    \maketitle

    \pagebreak

    \tableofcontents

    \pagebreak
    
    \part{Predicting Student Correctness}
    
    \section{K-Nearest Neighbor}
    \subsection{Complete Main kNN, Plot and Report Accuracy}
    The implementation of all code is in the \verb|part_a/knn.py| file. Following are plots of accuracy on validation data as a function of $k \in \{1,6,11,16,21,26\}$:

    \begin{pylabblock}[KNN]
        import codebase.part_a.knn as knn
        k_vals, val_user_acc, val_item_acc = knn.main("./codebase/data")
    \end{pylabblock}

    \begin{pylabblock}[KNN]
        knn.accuracy_plot(k_vals, val_user_acc, "User-Based Collaborative Filtering")
    \end{pylabblock}
    \pylab[KNN]{gen_figure(pytex)}

    \begin{pylabblock}[KNN]
        knn.accuracy_plot(k_vals, val_item_acc, "Item-Based Collaborative Filtering")
    \end{pylabblock}
    \pylab[KNN]{gen_figure(pytex)}

    


    \subsection{Selecting k*}
    
    \subsection{Implementing Impute by Item}
    Underlying assumption: if answers by certain users to Question A match those of Question B, then Aâ€™s answer correctness corresponding to a specific user matches that of question Y. 

    \subsection{Comparing user and item based Collaborative Filtering}
    User-Based collaborative filtering performs better on test data. $68.416\%$ accuracy on user-based filtering and $68.162\%$ accuracy on item-based filtering.

    \subsection{Potential Limitations of kNN in this Context}
    We can safely assume that there is a high correlation between both question difficulty and student ability on whether or not the question was answered correctly. But, feature importance is not possible for the KNN algorithm (there is no way to define the features which are responsible for the classification), so it will not be able to make accurate inferences based on these two parameters. In the algorithm used in this question, either one of the two parameters (user ability or question difficulty) is focused on, so it has lower validation and test accuracy scores than other algorithms in Part A of this project. 

    KNN runs slowly. Finding the optimal k-value from the given list of possible k values ({1, 6, 11, 21, 26}) takes several minutes for each function.

    


    \pagebreak

    \section{Item Response Theory}
    \subsection{Mathematical Derivations for IRT}
    We are given that $p(c_{ij} = 1 \vert \bm{\theta}, \bm{\beta})$. We will assume $c_{ij}$ is a value in $\bm{C}$ where $i$ and $j$ as coordinates are in set $O$ as defined:
    \[ O = \{(i,j): \text{Entry $(i,j)$ of matrix $\bm{C}$ and is observed}\} \].

    Since this $c_{ij}$ is a binary value, we can describe $P(\bm{C} \vert \bm{\theta}, \bm{\beta})$ with a bernoulli distribution:
    \[p(C \vert \bm{\theta}, \bm{\beta}) = \prod_{ij}[\frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}]^{c_{ij}}[\frac{1}{1 + exp(\theta_{i} - \beta_{j})}]^{(1-c_{ij})}\]

    Therefore, our Likelihood function is:
    \[L(\bm{\theta}, \bm{\beta}) =\prod_{ij}[\frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}]^{c_{ij}}[\frac{1}{1 + exp(\theta_{i} - \beta_{j})}]^{(1-c_{ij})}\]

    Then, apply log to obtain the log-likelihood where $N$ and $M$ are the number of users and questions respectively:
    \begin{align*}
        L(\bm{\theta}, \bm{\beta}) &=\prod_{ij}[\frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}]^{c_{ij}}[\frac{1}{1 + exp(\theta_{i} - \beta_{j})}]^{(1-c_{ij})} \\
        log(L(\bm{\theta}, \bm{\beta})) &= \log(\prod_{ij}[\frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}^{c_{ij}}][\frac{1}{1 + exp(\theta_{i} - \beta_{j})}^{1-c_{ij}}] \\
        &=\sum_{i=1}^{N} \sum_{j=1}^{M} \log([\frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}^{c_{ij}}][\frac{1}{1 + exp(\theta_{i} - \beta_{j})}^{1-c_{ij}}]) \\
        &=\sum_{i=1}^{N} \sum_{j=1}^{M} c_{ij}((log(exp(\theta_{i} - \beta_{j})) - log(1 + exp(\theta_{i} - \beta_{j}))) \\
        &+ (1 - c_{ij})(\log(1) - \log(1 + exp(\theta_{i} - \beta_{j}))) \\
        &=\sum_{i=1}^{N} \sum_{j=1}^{M} [c_{ij}(\theta_{i} - \beta_{j}) - \log(\frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})})] \\
    \end{align*}

    Then, we solve for the partial derivative with respect to $\theta_i$ and $\beta_j$ respectively:
    \begin{align*}
        \frac{\delta}{\delta\theta_{i}} &=  \sum_{j=1}^{M}{[c_{ij} - \frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}]} \\
        \frac{\delta}{\delta\beta_{j}} &= \sum_{i=1}^{N}{[-c_{ij} + \frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}]}
    \end{align*}
    
    \subsection{Implementation of IRT}
    The implementation of IRT is in \verb|part_a/item_response.py|. We chose the hyperparameters $\alpha$ and iterations number by performing multiple combinations of them and seeing which one had the highest validation score (automated, see mentioned code file for this automation). We then manually adjusted the set of tested values and repeated. Doing this a few times resulted in:

    \begin{pylabblock}[IRT]
        import codebase.part_a.item_response as irt
        print()
        irt_results = irt.main("./codebase/data")
    \end{pylabblock}
    \printpythontex[verb]

    \pylab[IRT]{gen_figure(pytex, fig_num=1)}

    \medskip

    \subsection{Reporting Validation and Test Accuracies}

    Validation and test accuracies have been calculated in the previous call to the main function. Again, implementation is in \verb|part_a/item_response.py|.

    \medskip

    \noindent
    Our validation accuracy:
    \begin{pylabblock}[IRT]
        print(irt_results["val_acc"])
    \end{pylabblock}
    \printpythontex[verb]

    \medskip

    \noindent
    Test accuracy:
    \begin{pylabblock}[IRT]
        print(irt_results["test_acc"])
    \end{pylabblock}
    \printpythontex[verb]

    \subsection{Plots of Questions With Respect to $\bm{\theta}$ and $\bm{\beta}$}

    \pylab[IRT]{gen_figure(pytex, fig_num=2)}

    \pagebreak

    \section{Neural Networks}
    \subsection{Differences Between ALS and Neural Networks}
    
    ALS optimizes 2 variable U and Z, neural net optimize one variable W(with gradient descent),

    ALS is an optimization algorithm that is incorporated as a part of a machine learning algorithm, while the neural network is a machine learning algorithm that that uses optimization algorithms to achieve learning.

    In neural net, W is used to manipulate x, while in ALS, W,X is being optimized as one variable U .

    ALS is essentially measuring the difference between target and product of two value(s), the obtain the two value, train matrix need to be pre-processed with SVD, neural network don't need to do this.

    neural net don't optimize latent Z directly but achieve Z's optimization with W_1 using g(W_1 x), where as ALS directly optimize Z.


    \subsection{Implementing AutoEncoder}
    This part can be found in \verb|part_a/neural_network.py|.

    \subsection{Tuning and Training NN}
    \begin{pylabblock}[NN]
        import codebase.part_a.neural_network as nn
        nn.main(data_path="./codebase/data", verbosity = 1)
    \end{pylabblock}
    \printpythontex[verb]

    \subsection{Plotting and Reporting}
    The following are the plots generated:

    \pylab[NN]{gen_figure(pytex, scale=0.7)}
    
    \subsection{Implementing $L_2$ Regularization}
    $L_2$ regularization has been implemented in the same code file as the other parts of this question (\verb|part_a/neural_network.py|).

    \begin{verbatim}
        lamb=0
        Final Validation Accuracy*    0.6858594411515665
        Final Test Accuracy*    0.6836014676827548
        lamb=0.001
        Final Validation Accuracy*    0.6869884278859724
        Final Test Accuracy*    0.6785210273779283
    \end{verbatim}

    \begin{verbatim}
        (optional extra finding):
        but with lamb=0.00025
        Final accuracy    0.6848715777589613
        Final Test Accuracy*    0.6861416878351679
    \end{verbatim}

    There are improvements on the validation accuracy, but not on the test accuracy.


    \pagebreak

    \section{Ensemble}
    Code for the ensemble is implemented in \verb|part_a/ensembly.py|. We bagged our base neural network, k-Nearest-Neighbor, and Item-Response models with their previously discussed optimal hyper parameters.

    \begin{pylabblock}[ENSEMBLE]
        import codebase.part_a.ensemble as ensemble
        ensemble.evaluate(verbosity=1, data_path="./codebase/data")
    \end{pylabblock}
    \printpythontex[verb]

    \pagebreak

    \part{Modifying for Higher Accuracy}
    \section{Formal Description}
    \section{Figure or Diagram}
    \section{Comparison or Demonstration}
\end{document}