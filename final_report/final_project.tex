\documentclass{article}
\usepackage[gobble=auto]{pythontex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{bbm}
\usepackage{graphicx}

\setpythontexfv{breaklines}
\renewcommand\thesubsection{\alph{subsection}}

\title{CSC311 - Final Assignment}
\author{Zelong Liu, Fizzah Mansoor, Harrison Deng}

\begin{document}
    \begin{pylabcode}
        import glob
        import shutil
        import os
        ### Setting up PythonTex environment. ###

        # Copy necessary code #
        for p in glob.glob("../code/**/*.py"):
            pytex.add_dependencies(p)
        
        if os.path.exists("./codebase"):
            shutil.rmtree("./codebase")
        shutil.copytree("../code/", "./codebase")
        for p in glob.glob("./codebase/**/*"):
            pytex.add_created(p)

        # Setting up automatic graphic inclusion #
        fig_info = {
            "dir": "./figures/generated/",
            "basename": '_'.join([pytex.session, pytex.family, pytex.restart]),
            "num": 0
        }
        pytex.figures = fig_info

        def gen_figure():
            figures = pytex.figures
            for num in get_fignums():
                name = figures["basename"] + "_fig" + str(figures["num"]) + ".pdf"
                path = figures["dir"] + name
                figures["num"] = figures["num"] + 1
                figure(figures["num"])
                savefig(path)
                pytex.add_created(path)
            return "\\includegraphics{" + path + "}"
    \end{pylabcode}


    \maketitle

    \pagebreak

    \tableofcontents

    \pagebreak
    
    \part{Predicting Student Correctness}
    
    \section{K-Nearest Neighbor}
    \subsection{Complete Main kNN, Plot and Report Accuracy}
    \subsection{Selecting k*}
    \subsection{Implementing Impute by Item}
    \subsection{Comparing user and item based Collaborative Filtering}
    \subsection{Potential Limitations of kNN in this Context}

    \pagebreak

    \section{Item Response Theory}
    \subsection{Mathematical Derivations for IRT}
    We are given that $p(c_{ij} = 1 \vert \bm{\theta}, \bm{\beta})$. We will assume $c_{ij}$ is a value in $\bm{C}$ where $i$ and $j$ as coordinates are in set $O$ as defined:
    \[ O = \{(i,j): \text{Entry $(i,j)$ of matrix $\bm{C}$ and is observed}\} \].

    Since this $c_{ij}$ is a binary value, we can describe $P(\bm{C} \vert \bm{\theta}, \bm{\beta})$ with a bernoulli distribution:
    \[p(C \vert \bm{\theta}, \bm{\beta}) = \prod_{ij}[\frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}]^{c_{ij}}[\frac{1}{1 + exp(\theta_{i} - \beta_{j})}]^{(1-c_{ij})}\]

    Therefore, our Likelihood function is:
    \[L(\bm{\theta}, \bm{\beta}) =\prod_{ij}[\frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}]^{c_{ij}}[\frac{1}{1 + exp(\theta_{i} - \beta_{j})}]^{(1-c_{ij})}\]

    Then, apply log to obtain the log-likelihood where $N$ and $M$ are the number of users and questions respectively:
    \begin{align*}
        L(\bm{\theta}, \bm{\beta}) &=\prod_{ij}[\frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}]^{c_{ij}}[\frac{1}{1 + exp(\theta_{i} - \beta_{j})}]^{(1-c_{ij})} \\
        log(L(\bm{\theta}, \bm{\beta})) &= \log(\prod_{ij}[\frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}^{c_{ij}}][\frac{1}{1 + exp(\theta_{i} - \beta_{j})}^{1-c_{ij}}] \\
        &=\sum_{i=1}^{N} \sum_{j=1}^{M} \log([\frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}^{c_{ij}}][\frac{1}{1 + exp(\theta_{i} - \beta_{j})}^{1-c_{ij}}]) \\
        &=\sum_{i=1}^{N} \sum_{j=1}^{M} c_{ij}((log(exp(\theta_{i} - \beta_{j})) - log(1 + exp(\theta_{i} - \beta_{j}))) \\
        &+ (1 - c_{ij})(\log(1) - \log(1 + exp(\theta_{i} - \beta_{j}))) \\
        &=\sum_{i=1}^{N} \sum_{j=1}^{M} [c_{ij}(\theta_{i} - \beta_{j}) - \log(\frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})})] \\
    \end{align*}

    Then, we solve for the partial derivative with respect to $\theta_i$ and $\beta_j$ respectively:
    \begin{align*}
        \frac{\delta}{\delta\theta_{i}} &=  \sum_{j=1}^{M}{[c_{ij} - \frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}]} \\
        \frac{\delta}{\delta\beta_{j}} &= \sum_{i=1}^{N}{[-c_{ij} + \frac{exp(\theta_{i} - \beta_{j})}{1+exp(\theta_{i} - \beta_{j})}]}
    \end{align*}
    
    \subsection{Implementation of IRT}
    The implementation of IRT is in \verb|part_a/item_response.py|. We chose the hyperparameters $\alpha$ and iterations number by performing multiple combinations of them and seeing which one had the highest validation score (automated, see mentioned code file for this automation). We then manually adjusted the set of tested values and repeated. Doing this a few times resulted in:

    \begin{pylabblock}
        import codebase.part_a.item_response as irt
        print()
        irt_results = irt.main("./codebase/data")
    \end{pylabblock}
    \printpythontex[verb]

    \medskip

    \noindent
    The following is a plot of the training curve (and the code used to generate it):
    \begin{pylabblock}
        plot(irt_results["train_nllks"], label="train")
        plot(irt_results["val_nllks"], label="valid")
        ylabel("Negative Log Likelihood")
        xlabel("Iteration")
        title("Neg Log Likelihood for Train and Validation Data")
        legend()
    \end{pylabblock}
    \pylab{gen_figure()}

    \subsection{Reporting Validation and Test Accuracies}

    Validation and test accuracies have been calculated in the previous call to the main function. Again, implementation is in \verb|part_a/item_response.py|.

    \medskip

    \noindent
    Our validation accuracy:
    \begin{pylabblock}
        print(irt_results["val_acc"])
    \end{pylabblock}
    \printpythontex[verb]

    \medskip

    \noindent
    Test accuracy:
    \begin{pylabblock}
        print(irt_results["test_acc"])
    \end{pylabblock}
    \printpythontex[verb]

    \subsection{Plots of Questions With Respect to $\bm{\theta}$ and $\bm{\beta}$}

    \pagebreak

    \section{Neural Networks}
    \subsection{Differences Between ALS and Neural Networks}
    \subsection{Implementing AutoEncoder}
    \subsection{Tuning and Training NN}
    \subsection{Ploting and Reporting}
    \subsection{Implementing $L_2$ Regularization}

    \pagebreak

    \section{Ensemble}

    \pagebreak

    \part{Modifying for Higher Accuracy}
    \section{Formal Description}
    \section{Figure or Diagram}
    \section{Comparison or Demonstration}
\end{document}